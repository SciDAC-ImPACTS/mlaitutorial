{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df632b-7031-4b8e-86ce-8273d255fef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install xarray pooch # for the dataset\n",
    "!pip install torch # for defining the model\n",
    "!pip install deephyper # for HPO\n",
    "!pip install nvidia-physicsnemo # for FNO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e1cf02-f799-45db-9969-9f32f6e0b8d4",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "Hyperparameters are non-trainable parameters determining the networks capacity and structure as well as data preprocessing and training strategies. For example, in the previous tutorials, the number of FNO blocks, number of Fourier modes, number of hidden dimension in the lift and projection operations are hyperparameters, largely affect the network performance.\n",
    "\n",
    "## Explore the impact of hyperparameters\n",
    "Feel free to adjust all the hyparameters defininng the training process and network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed1f83-e35e-4187-8e5d-df5f6da57a29",
   "metadata": {},
   "source": [
    "# Load and process the data\n",
    "The first thing to do is to identify the task and dataset, we will use the SST data from xarray and perform pixel regression tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907816e-b781-4742-8259-75728b0d1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicsnemo.models.fno import FNO\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "# load and prepare the data\n",
    "ds = xr.tutorial.open_dataset('ersstv5')\n",
    "sst = ds['sst']\n",
    "\n",
    "\n",
    "dummy_value = 0.0\n",
    "# sst = sst.fillna(dummy_value)  \n",
    "\n",
    "\n",
    "# shape = (T, H, W)\n",
    "data = sst.values.astype('float32')\n",
    "T, H, W = data.shape\n",
    "\n",
    "# 2. Define sliding‚Äêwindow parameters\n",
    "T_in   = 1                     # number of input time steps\n",
    "T_pred = 1              # number of steps to predict\n",
    "step   = 1                       # sliding window stride\n",
    "# total number of (input, target) pairs\n",
    "n_samples = (T - (T_in + T_pred)) // step + 1\n",
    "\n",
    "# 3. Build arrays of shape (n_samples, H, W, T_in) and (n_samples, H, W, T_pred)\n",
    "X = np.zeros((n_samples, T_in, H, W),  dtype='float32')\n",
    "Y = np.zeros((n_samples, T_pred, H, W), dtype='float32')\n",
    "\n",
    "for i in range(n_samples):\n",
    "    start = i * step\n",
    "    X[i] = data[start : start + T_in]\n",
    "    Y[i] = data[start + T_in : start + T_in + T_pred]\n",
    "    \n",
    "# land masks\n",
    "x_mask_land = np.isnan(X[0])\n",
    "y_mask_land = np.isnan(Y[0])\n",
    "\n",
    "# fill nans\n",
    "X[:,x_mask_land] = dummy_value\n",
    "Y[:,x_mask_land] = dummy_value\n",
    "\n",
    "print(\"Data shapes are\", X.shape, Y.shape)\n",
    "\n",
    "\n",
    "# 5. Split into train / eval / test\n",
    "ntrain = int(0.8 * n_samples)\n",
    "neval  = int(0.2 * ntrain)\n",
    "ntest  = n_samples - ntrain\n",
    "\n",
    "# train\n",
    "train_X = torch.from_numpy(X[:ntrain])\n",
    "train_Y = torch.from_numpy(Y[:ntrain])\n",
    "# eval (from end of train split)\n",
    "eval_X = torch.from_numpy(X[ntrain : ntrain+neval])\n",
    "eval_Y = torch.from_numpy(Y[ntrain : ntrain+neval])\n",
    "# test (the remainder)\n",
    "test_X = torch.from_numpy(X[ntrain+neval :])\n",
    "test_Y = torch.from_numpy(Y[ntrain+neval :])\n",
    "\n",
    "\n",
    "# 6. Create DataLoaders\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_X, train_Y),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "eval_loader  = DataLoader(TensorDataset(eval_X,  eval_Y),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(TensorDataset(test_X,  test_Y),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73343eb-dcc9-4447-b718-0d535d1c2945",
   "metadata": {},
   "source": [
    "# Train a model and adjust hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9adaf32-3e7e-48ea-9d76-30371a3d57a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "model = FNO(in_channels=1, out_channels=1, num_fno_modes=6, dimension=2) \n",
    "model.to(device)\n",
    "\n",
    "# start traning\n",
    "EPOCHS = 50\n",
    "LR = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "\n",
    "# ocean masks\n",
    "x_mask = torch.tensor(~x_mask_land, device=device)\n",
    "y_mask = torch.tensor(~y_mask_land, device=device)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_stats = {\"train_loss\": [], \"val_loss\": []}\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Training...\"):\n",
    "    model.train()\n",
    "    running_train_ls = 0\n",
    "    for x, y in train_loader:\n",
    "        # send to device and zero out land areas\n",
    "        x = x.to(device) * x_mask\n",
    "        y = y.to(device) * y_mask\n",
    "        y_pred = model(x) * y_mask\n",
    "        assert y_pred.shape == y.shape, f\"shape mismatch, y_pred shape {y_pred.shape}\"\n",
    "        train_loss = torch.nn.functional.mse_loss(y, y_pred)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_ls += train_loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    running_val_ls = 0\n",
    "    for x_val, y_val in eval_loader:\n",
    "        x_val = x_val.to(device) * x_mask\n",
    "        y_val = y_val.to(device) * y_mask\n",
    "        y_pred_val = model(x_val) * y_mask\n",
    "        val_loss = torch.nn.functional.mse_loss(y_val, y_pred_val)\n",
    "        running_val_ls += val_loss.item()\n",
    "\n",
    "    train_stats[\"train_loss\"].append(running_train_ls / len(train_loader))\n",
    "    train_stats[\"val_loss\"].append(running_val_ls/ len(eval_loader))\n",
    "        \n",
    "# visualize learning curves\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_box_aspect(1/1.62)\n",
    "ax.plot(np.arange(1,  EPOCHS+1), train_stats[\"train_loss\"], label=\"Train\")\n",
    "ax.plot(np.arange(1, EPOCHS+1), train_stats[\"val_loss\"], label=\"Validation\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "\n",
    "print(f\"Final validation loss is {train_stats['val_loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b549c-30a8-493c-b6cb-4ba0bc208d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize baseline prediction\n",
    "model.eval()\n",
    "x_test, y_test = next(iter(test_loader))\n",
    "x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "x_test = x_test * x_mask\n",
    "y_pred = model(x_test) * y_mask\n",
    "\n",
    "\n",
    "plot_mask = y_mask.detach().cpu().numpy()\n",
    "plot_mask = plot_mask.astype(bool)\n",
    "\n",
    "\n",
    "\n",
    "y_true = y_test.detach().cpu().numpy()[0]\n",
    "y_pred = y_pred.detach().cpu().numpy()[0]\n",
    "y_true[~plot_mask] = np.nan\n",
    "y_pred[~plot_mask] = np.nan\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6))\n",
    "for a in ax:\n",
    "    a.set_box_aspect(1/1.62)\n",
    "    a.axis('off')\n",
    "ax[0].imshow(y_true.squeeze())\n",
    "ax[0].set_title(\"True\")\n",
    "ax[1].imshow(y_pred.squeeze())\n",
    "ax[1].set_title(\"Predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2acc6e7-7933-4dfe-8297-a4fecbd7324f",
   "metadata": {},
   "source": [
    "# Hyperparameter search\n",
    "It is natural to ask the quetion: how can we find the best set of hyperparameters that it can lead to the best performing model for the task?\n",
    "\n",
    "We can:\n",
    "- Exhaust all the combination of hyperparameters and conduct evaluation and pick out the best performing one.\n",
    "- Randomly select a few sets of hyparameters and do the evaluation and return the best performing one.\n",
    "- Determine the most influential subset of hyperparameters that much smaller than the entire set and perform the search.\n",
    "\n",
    "**Is there a better and more efficient way?**\n",
    "- Genetic algorithm\n",
    "- Bayesian optimization\n",
    "- ...\n",
    "\n",
    "We will breifly discuss how Bayesian optimization can help with the task and the types of surrogates we can use for this task.\n",
    "\n",
    "## Bayesian optimization (BO) with DeepHyper\n",
    "BO is a global black-box function optimization method where we build probablistic models of the function and iteratively evaluation promising function inputs based on the current model.\n",
    "\n",
    "![hi](./imgs/BO_demonstration.png)\n",
    "\n",
    "With this\n",
    "- hyperparameters as the input\n",
    "- model evaluation metrics as the output (e.g., validation loss)\n",
    "\n",
    "## Acquisition function\n",
    "Upper confidence bound\n",
    "$$\n",
    "\\alpha(x) = \\mu(x) + \\lambda \\sigma(x)\n",
    "$$\n",
    "\n",
    "\n",
    "<u>DeepHyper</u> provides efficient and scalable BO framework for hyperparameter search in deep learning models.\n",
    "\n",
    "To perform hyperparameter search in DeepHyper, we need\n",
    "- a learning tasks and training and validation datasets\n",
    "- a hyperparameter search space where an element of the space determines a neural network and its training process\n",
    "- a run function that takes as input the hyperparameters and outputs the objective\n",
    "- an evalulation to excute the search with pre-defined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895c196-2425-45a4-88b3-7c0784dce846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some deephyper imports\n",
    "# briefly explain each API\n",
    "from deephyper.hpo import HpProblem\n",
    "from deephyper.evaluator import Evaluator\n",
    "from deephyper.hpo import CBO\n",
    "from deephyper.evaluator import RunningJob, profile # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fdf5f-5e2f-48c8-827b-db3964562d6a",
   "metadata": {},
   "source": [
    "# Define the hyperparameter search space\n",
    "Hyperparameter ranges are defined using the following syntax:\n",
    "\n",
    "- Discrete integer ranges are generated from a tuple (lower: `int`, upper:`int`); e.g., number of layers, number of neurons per layer, and number of traning steps, etc.\n",
    "\n",
    "- Continuous prarameters are generated from a tuple (lower: `float`, upper: `float`); e.g., learning rate, weight decay, drop out rate, etc.\n",
    "\n",
    "- Categorical or nonordinal hyperparameter ranges can be given as a list of possible values `[val1, val2, ...]`; e.g., types of activation functions, data normalization strategies, etc.\n",
    "\n",
    "We provide the default configuration of hyperparameters as a starting point of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588fd8a-d2de-4dc4-bf7e-176c5e1085b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hpproblem\n",
    "problem = HpProblem()\n",
    "activations = [\n",
    "    \"relu\",\n",
    "    \"leaky_relu\",\n",
    "    \"prelu\",\n",
    "    \"relu6\",\n",
    "    \"elu\",\n",
    "    \"selu\",\n",
    "    \"silu\",\n",
    "    \"gelu\",\n",
    "    \"sigmoid\",\n",
    "    \"logsigmoid\",\n",
    "    \"softplus\",\n",
    "    \"softshrink\",\n",
    "    \"softsign\",\n",
    "    \"tanh\",\n",
    "    \"tanhshrink\",\n",
    "    \"threshold\",\n",
    "    \"hardtanh\",\n",
    "    \"identity\",\n",
    "    \"squareplus\",\n",
    "]\n",
    "optimizers = [\"Adadelta\", \"Adagrad\", \"Adam\", \"AdamW\", \"RMSprop\", \"SGD\"]\n",
    "\n",
    "problem.add_hyperparameter((1, 16), \"padding\", default_value=8)\n",
    "problem.add_hyperparameter(\n",
    "    [\"constant\", \"reflect\", \"replicate\", \"circular\"],\n",
    "    \"padding_type\",\n",
    "    default_value=\"constant\",\n",
    ")\n",
    "problem.add_hyperparameter([True, False], \"coord_feat\", default_value=True)\n",
    "problem.add_hyperparameter(activations, \"lift_act\", default_value=\"gelu\")\n",
    "problem.add_hyperparameter((2, 32), \"num_FNO\", default_value=4)\n",
    "problem.add_hyperparameter((2, 32), \"num_modes\", default_value=3)\n",
    "problem.add_hyperparameter((2, 64), \"latent_ch\", default_value=32)\n",
    "problem.add_hyperparameter((1, 16), \"num_projs\", default_value=1)\n",
    "problem.add_hyperparameter((2, 32), \"proj_size\", default_value=32)\n",
    "problem.add_hyperparameter(activations, \"proj_act\", default_value=\"silu\")\n",
    "\n",
    "# problem.add_hyperparameter(optimizers, \"optimizer\", default_value=\"Adam\")\n",
    "problem.add_hyperparameter(\n",
    "    (1e-6, 1e-2, \"log-uniform\"), \"lr\", default_value=1e-3\n",
    ")\n",
    "problem.add_hyperparameter((0.0, 0.1), \"weight_decay\", default_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c3077-2a0a-49a2-9b59-4a0954a9b2a3",
   "metadata": {},
   "source": [
    "# Define the `run` function\n",
    "In Deephyper, the `run` function takes a `RunningJob` as the input and output the objective or a dictionary containing the objective and optional meta data. The `run` function can be regarded as the black-box function evaluation step in the optimization process. The training loop can be defined inside or outside the run function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c12b2f-8f56-4e07-852f-a2142dff10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "@profile\n",
    "def run(job: RunningJob):\n",
    "    config = job.parameters\n",
    "    config = SimpleNamespace(**config)\n",
    "    model = FNO(\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        decoder_layers=config.num_projs,\n",
    "        decoder_layer_size=config.proj_size,\n",
    "        decoder_activation_fn=config.proj_act,\n",
    "        dimension=2,\n",
    "        latent_channels=config.latent_ch,\n",
    "        num_fno_layers=config.num_FNO,\n",
    "        num_fno_modes=int(config.num_modes),\n",
    "        padding=config.padding,\n",
    "        padding_type=config.padding_type,\n",
    "        activation_fn=config.lift_act,\n",
    "        coord_features=config.coord_feat,\n",
    "    ).to(device)\n",
    "\n",
    "    # start traning\n",
    "    EPOCHS = 2\n",
    "    LR = config.lr\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=config.weight_decay)\n",
    "\n",
    "    train_stats = {\"train_loss\": [], \"val_loss\": []}\n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        model.train()\n",
    "        running_train_ls = 0\n",
    "        for x, y in train_loader:\n",
    "            # send to device and zero out land areas\n",
    "            x = x.to(device) * x_mask\n",
    "            y = y.to(device) * y_mask\n",
    "            y_pred = model(x) * y_mask\n",
    "            assert y_pred.shape == y.shape, f\"shape mismatch, y_pred shape {y_pred.shape}\"\n",
    "            train_loss = torch.nn.functional.mse_loss(y, y_pred)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_ls += train_loss.item()\n",
    "    \n",
    "        model.eval()\n",
    "        running_val_ls = 0\n",
    "        for x_val, y_val in eval_loader:\n",
    "            x_val = x_val.to(device) * x_mask\n",
    "            y_val = y_val.to(device) * y_mask\n",
    "            y_pred_val = model(x_val) * y_mask\n",
    "            val_loss = torch.nn.functional.mse_loss(y_val, y_pred_val)\n",
    "            running_val_ls += val_loss.item()\n",
    "    \n",
    "        train_stats[\"train_loss\"].append(running_train_ls / len(train_loader))\n",
    "        train_stats[\"val_loss\"].append(running_val_ls/ len(eval_loader))\n",
    "        obj = -float(train_stats[\"val_loss\"][-1])\n",
    "    return {\"objective\": obj, \"metadata\":{\"train_loss\": train_stats[\"train_loss\"], \"validation_loss\": train_stats[\"val_loss\"]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7604c7e-933a-4737-a894-46c8aa3e7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.evaluator import Evaluator\n",
    "from deephyper.evaluator.callback import TqdmCallback\n",
    "\n",
    "\n",
    "# define the evaluator to distribute the computation\n",
    "evaluator = Evaluator.create(\n",
    "    run,\n",
    "    method=\"thread\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f781c52-e2df-4991-a3e4-f0826b0e60fd",
   "metadata": {},
   "source": [
    "# Execute the search with Centralized BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c807894-0425-4c6a-962e-50e0040729ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.hpo import CBO\n",
    "search = CBO(problem, evaluator, verbose=1,\n",
    "        initial_points=[problem.default_configuration], random_state=0)\n",
    "reuslts = search.search(max_evals=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033c1a4-e4ba-409a-8c30-d1688e00cd10",
   "metadata": {},
   "source": [
    "# Inspect the search reults and return the optimial hyperparameters\n",
    "DeepHyper offers utilities to conduct simple visualization of the search trajectories to check if the search has been effective. During the search process, we expect to see improvement on the search objective. After the search, we can finally return the optimial configuration and optionally retrain the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa3fc8-fcdd-42f7-b596-d711cab47008",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b46a71a-75e7-4a72-960e-739f70666761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.analysis.hpo import plot_search_trajectory_single_objective_hpo\n",
    "\n",
    "\n",
    "WIDTH_PLOTS = 8\n",
    "HEIGHT_PLOTS = WIDTH_PLOTS / 1.618\n",
    "\n",
    "baseline = results[results.job_id == 0]['objective'].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(WIDTH_PLOTS, HEIGHT_PLOTS))\n",
    "ax.axhline(y=-baseline_score, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
    "plot_search_trajectory_single_objective_hpo(reuslts, mode=\"min\", ax=ax)\n",
    "_ = plt.title(\"Search Trajectory\")\n",
    "_ = plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b87a6-5268-4313-8b98-d8fb42386d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
